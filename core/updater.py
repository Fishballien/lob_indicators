# -*- coding: utf-8 -*-
"""
Created on Thu Oct 17 16:12:06 2024

@author: Xintang Zheng

æ˜Ÿæ˜Ÿ: â˜… â˜† âœª âœ© ğŸŒŸ â­ âœ¨ ğŸŒ  ğŸ’« â­ï¸
å‹¾å‹¾å‰å‰: âœ“ âœ” âœ• âœ– âœ… â
æŠ¥è­¦å•¦: âš  â“˜ â„¹ â˜£
ç®­å¤´: â” âœ â™ â¤ â¥ â†© â†ª
emoji: ğŸ”” â³ â° ğŸ”’ ğŸ”“ ğŸ›‘ ğŸš« â— â“ âŒ â­• ğŸš€ ğŸ”¥ ğŸ’§ ğŸ’¡ ğŸµ ğŸ¶ ğŸ§­ ğŸ“… ğŸ¤” ğŸ§® ğŸ”¢ ğŸ“Š ğŸ“ˆ ğŸ“‰ ğŸ§  ğŸ“

"""
# %% imports
import os
from pathlib import Path
import pandas as pd
import numpy as np
import toml
import json
import traceback
from datetime import timedelta, datetime
from concurrent.futures import ThreadPoolExecutor, as_completed


from utils.logutils import FishStyleLogger
from utils.dirutils import get_filenames_by_extension, load_path_config
from core.processor import *
from core.ft2ts2cs import ConcatProcessor


# %% incremental
class IncrementalUpdate:
    
    dt_format = '%Y-%m-%d'
    
    def __init__(self, incremental_name):
        self.incremental_name = incremental_name
        
        self._load_path_config()
        self._init_dir()
        self._load_params()
        self._init_log()
        
    def _load_path_config(self):
        file_path = Path(__file__).resolve()
        project_dir = file_path.parents[1]
        self.path_config = load_path_config(project_dir)
        
    def _init_dir(self):
        self.param_dir = Path(self.path_config['param'])
        self.flag_dir = Path(self.path_config['flag']) / self.incremental_name
        self.flag_dir.mkdir(parents=True, exist_ok=True)
        
    def _load_params(self):
        self.params = toml.load(self.param_dir / 'inc' / f'{self.incremental_name}.toml')
        
    def _init_log(self):
        self.log = FishStyleLogger()
        
    def run(self, date_today):
        date_today = datetime.now().date() if date_today is None else datetime.strptime(date_today, '%Y%m%d')
        self._get_start_date(date_today)
        self._get_end_date(date_today)
        self.log.info(f'Update {self.start_date} ~ {self.end_date}')
        self.msgs = []
        self._load_flags(date_today)
        self._inc_update_all_indicators()
        self._save_flags(date_today)
        self._check_update_status(date_today)
        
    def _get_start_date(self, date_today):
        start_date = self.params.get('start_date')
        if start_date is None:
            lookback = timedelta(**self.params['lookback'])
            start_date = (date_today - lookback).strftime(self.dt_format)
        self.start_date = start_date
        
    def _get_end_date(self, date_today):
        end_date = self.params.get('end_date')
        if end_date is None:
            lookback = timedelta(**self.params['delay'])
            end_date = (date_today - lookback).strftime(self.dt_format)
        self.end_date = end_date
        
    def _load_flags(self, date_today):
        indicators = [ind_info['ind_ver_name'] for ind_info in self.params['indicators']]
        date_in_fmt = date_today.strftime(self.dt_format)
        flag_file_name = f'{date_in_fmt}.json'
        self.flag_path = self.flag_dir / flag_file_name
        if os.path.exists(self.flag_path):
            with open(self.flag_path, 'r') as f:
                self.flags = json.load(f)
        else:
            self.flags = {ind: False for ind in indicators}
        
    def _inc_update_all_indicators(self):
        indicators = self.params['indicators']

        for indicator_info in indicators:
            ind_ver_name = indicator_info['ind_ver_name']
            if self.flags[ind_ver_name]:
                continue
            processor_name = indicator_info['processor_name']
            gen_status = self._generate_one(ind_ver_name, processor_name)
            if gen_status == 0:
                trans_status = self._trans_one(ind_ver_name)
                if trans_status == 0:
                    self.flags[ind_ver_name] = True
                else:
                    self.msgs.append({
                        'type': 'msg',
                        'content': {
                            'level': 'error',
                            'title': f'trans {ind_ver_name} error',
                            'msg': trans_status,
                            }
                        })
            else:
                self.msgs.append({
                    'type': 'msg',
                    'content': {
                        'level': 'error',
                        'title': f'generate {ind_ver_name} error',
                        'msg': gen_status,
                        }
                    })
            
    def _generate_one(self, ind_ver_name, processor_name):
        self.log.info(f'Start generating {ind_ver_name}')
        
        generate_n_workers = self.params['generate_n_workers']
        task_n_group = self.params['task_n_group']
        save_n_group = self.params['save_n_group']
        
        processor_class = globals()[processor_name]
        processor = processor_class(
            ind_ver_name=ind_ver_name,
            start_date=self.start_date,
            end_date=self.end_date,
            n_workers=generate_n_workers,
            task_n_group=task_n_group, 
            save_n_group=save_n_group, 
            mode='update',
        )
        
        try:
            status = processor.run()
            if status == 0:
                self.log.success(f'Finished generating {ind_ver_name}')
            return status
        except:
            e_format = traceback.format_exc()
            self.log.exception(f'Error generating {ind_ver_name}')
            return e_format
       
        
    def _trans_one(self, ind_ver_name):
        self.log.info(f'Start transforming {ind_ver_name}')
        
        trans_n_workers = self.params.get('trans_n_workers')
        if trans_n_workers is None:
            return 0
        
        processor = ConcatProcessor(
            ind_ver_name=ind_ver_name,
            n_workers=trans_n_workers,
            mode='update',
        )
        
        try:
            processor.run()
            self.log.success(f'Finished transforming {ind_ver_name}')
            return 0
        except:
            e_format = traceback.format_exc()
            self.log.exception(f'Error transforming {ind_ver_name}')
            return e_format
        
    def _save_flags(self, date_today):
        with open(self.flag_path, 'w') as f:
            json.dump(self.flags, f)
            
    def _check_update_status(self, date_today):
        if all(self.flags.values()):
            self.msgs.append({
                'type': 'update',
                'content': {
                    'obj': f'ind_inc_{self.incremental_name}',
                    'data_ts': date_today,
                    }
                })
        

# %% update
def format_timedelta_threshold(timedelta_threshold):
    """æ ¼å¼åŒ–æ—¶é—´é˜ˆå€¼ï¼Œæ ¹æ®å¤©æ•°æˆ–å°æ—¶è¾“å‡ºåˆé€‚çš„æè¿°"""
    if timedelta_threshold.days > 0:
        return f"{timedelta_threshold.days}å¤©"
    elif timedelta_threshold.seconds >= 3600:
        hours = timedelta_threshold.seconds // 3600
        return f"{hours}å°æ—¶"
    elif timedelta_threshold.seconds >= 60:
        minutes = timedelta_threshold.seconds // 60
        return f"{minutes}åˆ†é’Ÿ"
    else:
        return f"{timedelta_threshold.seconds}ç§’"


def print_daily_diff_stats(difference_mask, combined_index, comparison_df):
    """æŒ‰å¤©èšåˆå¹¶è¿”å›æ¯å¤©ä¸ä¸€è‡´è¡Œçš„æ•°é‡ã€æ€»è¡Œæ•°ï¼Œä»¥åŠæ¯è¡Œå·®å¼‚æ¯”ä¾‹åœ¨ä¸€å¤©ä¸­çš„å‡å€¼ï¼Œæ ¼å¼ä¸ºå­—å…¸"""
    diff_summary = {}
    if difference_mask.any():
        # å°†ç´¢å¼•è½¬æ¢ä¸ºæ—¥æœŸï¼Œå¹¶ç»Ÿè®¡æ¯ä¸€å¤©ä¸ä¸€è‡´çš„è¡Œæ•°
        diff_df = pd.DataFrame({'diff': difference_mask}, index=combined_index)
        diff_df['date'] = diff_df.index.date
        
        daily_diff_count = diff_df.groupby('date')['diff'].sum()  # æ¯å¤©ä¸ä¸€è‡´çš„æ•°é‡
        daily_total_count = diff_df.groupby('date')['diff'].count()  # æ¯å¤©çš„æ€»æ•°
        
        # å¯¹æ¯ä¸€åˆ—æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œç„¶åè®¡ç®—æ¯è¡Œçš„æ¯”ä¾‹å‡å€¼
        row_diff_mean_by_day = comparison_df.groupby(comparison_df.index.date).mean()  # æŒ‰æ—¥æœŸè®¡ç®—æ¯åˆ—çš„å‡å€¼
        row_diff_mean = row_diff_mean_by_day.mean(axis=1)  # å¯¹æ¯è¡Œè®¡ç®—å‡å€¼
        
        # ç”Ÿæˆå·®å¼‚å­—å…¸ï¼ŒåŒ…å«æ¯æ—¥ä¸ä¸€è‡´æ¯”ä¾‹å’Œæ¯è¡Œå·®å¼‚æ¯”ä¾‹çš„å‡å€¼
        for date, diff_count in daily_diff_count.items():
            if diff_count > 0:
                total_count = daily_total_count.loc[date]
                daily_mean = row_diff_mean.loc[date]  # è·å–å½“å¤©æ¯è¡Œå·®å¼‚æ¯”ä¾‹çš„å‡å€¼
                diff_summary[date.strftime('%Y-%m-%d')] = (f"{int(diff_count)}/{total_count}", round(daily_mean, 5))
    
    return diff_summary


def add_dataframe_to_dataframe_reindex(df, new_data):
    """
    ä½¿ç”¨ reindex å°†æ–° DataFrame çš„æ•°æ®æ·»åŠ åˆ°ç›®æ ‡ DataFrame ä¸­ï¼Œæ”¯æŒåŠ¨æ€æ‰©å±•åˆ—å’Œè¡Œï¼ŒåŸå…ˆæ²¡æœ‰å€¼çš„åœ°æ–¹å¡«å…… NaNã€‚

    å‚æ•°:
    df (pd.DataFrame): ç›®æ ‡ DataFrameã€‚
    new_data (pd.DataFrame): è¦æ·»åŠ çš„æ–° DataFrameã€‚

    è¿”å›å€¼:
    df (pd.DataFrame): æ›´æ–°åçš„ DataFrameã€‚
    """
    # åŒæ—¶æ‰©å±•è¡Œå’Œåˆ—ï¼Œå¹¶ç¡®ä¿æœªå¡«å……çš„ç©ºå€¼ä¸º NaNï¼ŒæŒ‰æ’åº
    df = df.reindex(index=df.index.union(new_data.index, sort=True),
                    columns=df.columns.union(new_data.columns, sort=True),
                    fill_value=np.nan)
    
    # ä½¿ç”¨ loc æ·»åŠ æ–°æ•°æ®
    df.loc[new_data.index, new_data.columns] = new_data

    return df


class CheckNUpdate:
    
    def __init__(self, historical_dir, incremental_dir, updated_dir, params={}, n_workers=1, log=None):
        self.historical_dir = historical_dir
        self.incremental_dir = incremental_dir
        self.updated_dir = updated_dir
        self.params = params
        self.n_workers = n_workers
        self.log = log
        
        self._preprocess_params()
        
    def _preprocess_params(self):
        params = self.params
        timedelta_threshold = params.get('timedelta_threshold', {'minutes': 0})
        params['timedelta_threshold'] = timedelta(**timedelta_threshold)
        params['precision'] = params.get('precision', 1e-5)
        
    def run(self):
        self._get_factor_names()
        self._decide_pre_update_dir()
        self._update_all_factors()
        
    def _get_factor_names(self):
        self.factor_names = get_filenames_by_extension(self.historical_dir, '.parquet')
        
    def _decide_pre_update_dir(self):
        updated_factor_names = get_filenames_by_extension(self.updated_dir, '.parquet')

        len_his = len(self.factor_names)
        len_updated = len(updated_factor_names)
        if len_updated < len_his:
            self.pre_updated_dir = self.historical_dir
            self.log.warning(f'æ›´æ–°æ–‡ä»¶æ•°é‡ä¸è¶³ {len_updated}/{len_his}ï¼Œä½¿ç”¨å†å²æ–‡ä»¶åšæ›´æ–°ï¼')
        else:
            self.pre_updated_dir = self.updated_dir
            
# =============================================================================
#     def _update_all_factors(self):
#         for factor_name in self.factor_names:
#             try:
#                 self._update_one_factor(factor_name)
#             except:
#                 pass
# =============================================================================
            
    def _update_all_factors(self):
        """ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘æ›´æ–°æ‰€æœ‰å› å­"""
        len_of_factors = len(self.factor_names)
        with ThreadPoolExecutor(max_workers=min(self.n_workers, len_of_factors)) as executor:
            futures = {executor.submit(self._update_one_factor, factor_name): 
                       factor_name for factor_name in self.factor_names}
            
            for future in as_completed(futures):
                factor_name = futures[future]
                try:
                    future.result()  # è·å–çº¿ç¨‹çš„è¿”å›ç»“æœï¼Œç¡®ä¿å¼‚å¸¸è¢«æ•è·
                except Exception as exc:
                    self.log.error(f'{factor_name} æ›´æ–°è¿‡ç¨‹ä¸­å‡ºé”™: {exc}')
    
    def _update_one_factor(self, factor_name):
        pre_updated_dir = self.pre_updated_dir
        incremental_dir = self.incremental_dir
        updated_dir = self.updated_dir

        file_name = f'{factor_name}.parquet'
        
        pre_update_path = pre_updated_dir / file_name
        incremental_path = incremental_dir / file_name
        pre_update_data = pd.read_parquet(pre_update_path)
        incremental_data = pd.read_parquet(incremental_path)
        updated_data = self._check_n_update(factor_name, pre_update_data, incremental_data)
        
        updated_path = updated_dir / file_name
        updated_data.to_parquet(updated_path)
    
    def _check_n_update(self, factor_name, pre_update_data, incremental_data):
        timedelta_threshold = self.params['timedelta_threshold']
        
        # æ£€æŸ¥æ•°æ®é‡å 
        pre_start, pre_end, inc_start, inc_end = self._check_data_overlap(
            factor_name, pre_update_data, incremental_data)

        # å¤„ç† before_threshold æ•°æ®å·®å¼‚
        self._process_threshold_data(factor_name, pre_update_data, incremental_data, inc_start, 
                                     is_before_threshold=True)
        
        # å¤„ç† after_threshold æ•°æ®å·®å¼‚ï¼Œåªåˆ° pre_end
        self._process_threshold_data(factor_name, pre_update_data, incremental_data, inc_start, 
                                     is_before_threshold=False, pre_end=pre_end)
        
        updated_data = self._update_to_updated(pre_update_data, incremental_data, 
                                               inc_start, timedelta_threshold, pre_end)
        
        return updated_data
    
    def _check_data_overlap(self, factor_name, pre_update_data, incremental_data):
        pre_start = pre_update_data.index[0]
        pre_end = pre_update_data.index[-1]
        inc_start = incremental_data.index[0]
        inc_end = incremental_data.index[-1]
        
        if inc_end <= pre_start or inc_start >= pre_end:
            self.log.error(f"{factor_name} æ•°æ®æ²¡æœ‰é‡å éƒ¨åˆ†")
            raise
        
        return pre_start, pre_end, inc_start, inc_end

    def _process_threshold_data(self, factor_name, pre_update_data, incremental_data, inc_start, is_before_threshold, 
                               pre_end=None):
        timedelta_threshold = self.params['timedelta_threshold']
        precision = self.params['precision']
        
        threshold_time = inc_start + timedelta_threshold
        timedelta_threshold_in_format = format_timedelta_threshold(timedelta_threshold)
        if is_before_threshold:
            inc_data = incremental_data.loc[inc_start:threshold_time]  # ä¿®æ­£é€»è¾‘ï¼Œfrom inc_start å¼€å§‹
            pre_data = pre_update_data.loc[inc_start:threshold_time]
            threshold_desc = f"å‰{timedelta_threshold_in_format}"
        else:
            inc_data = incremental_data.loc[threshold_time:pre_end]  # ä¿®æ­£é€»è¾‘ï¼Œå¤„ç†åˆ° pre_end
            pre_data = pre_update_data.loc[threshold_time:pre_end]
            threshold_desc = f"å‰{timedelta_threshold_in_format}åçš„"
        
        combined_index = pre_data.index.union(inc_data.index)
        combined_columns = pre_data.columns.union(inc_data.columns)
        
        pre_data = pre_data.reindex(index=combined_index, columns=combined_columns, fill_value=np.nan)
        inc_data = inc_data.reindex(index=combined_index, columns=combined_columns, fill_value=np.nan)
        
        comparison = ~np.isclose(pre_data, inc_data, atol=precision, equal_nan=True)
        difference_rows = comparison.any(axis=1)
        comparison_df = pd.DataFrame(comparison, index=combined_index, columns=combined_columns)
        
        diff_summary = print_daily_diff_stats(difference_rows, combined_index, comparison_df)
        
        if diff_summary:
            self.log.warning(f"{factor_name}{threshold_desc}æ•°æ®å·®å¼‚: {diff_summary}")
    
    def _update_to_updated(self, pre_update_data, incremental_data, inc_start, timedelta_threshold, pre_end):
        updated_data = add_dataframe_to_dataframe_reindex(
            pre_update_data, incremental_data.loc[(inc_start+timedelta_threshold):])
        
        return updated_data
    
    
# %%
if __name__=='__main__':
    ind_ver_name = "indv9_slope_hl"
    processor_name = 'IndicatorProcessorByL2'
    exchange = "usd"
    
    lob_exchange_dir = Path(r'D:\mnt\Data\Crypto\ProcessedData\lob_shape\usd')
    update_params = {
        'presicion': 1e-5,
        'timedelta_threshold': {
            'days': 0
            },
        }
    n_workers = 2
    
    ind_ver_dir = lob_exchange_dir / ind_ver_name
    historical_dir = ind_ver_dir / 'cs'
    incremental_dir = ind_ver_dir / 'incremental_cs'
    updated_dir = ind_ver_dir / 'updated_cs'
    updated_dir.mkdir(exist_ok=True, parents=True)
    
    log = FishStyleLogger()

    updater = CheckNUpdate(historical_dir, incremental_dir, updated_dir, params=update_params, n_workers=n_workers, log=log)
    updater.run()
# -*- coding: utf-8 -*-
"""
Created on Wed Nov 20 16:47:10 2024

@author: Xintang Zheng

æ˜Ÿæ˜Ÿ: â˜… â˜† âœª âœ© ğŸŒŸ â­ âœ¨ ğŸŒ  ğŸ’« â­ï¸
å‹¾å‹¾å‰å‰: âœ“ âœ” âœ• âœ– âœ… â
æŠ¥è­¦å•¦: âš  â“˜ â„¹ â˜£
ç®­å¤´: â” âœ â™ â¤ â¥ â†© â†ª
emoji: ğŸ”” â³ â° ğŸ”’ ğŸ”“ ğŸ›‘ ğŸš« â— â“ âŒ â­• ğŸš€ ğŸ”¥ ğŸ’§ ğŸ’¡ ğŸµ ğŸ¶ ğŸ§­ ğŸ“… ğŸ¤” ğŸ§® ğŸ”¢ ğŸ“Š ğŸ“ˆ ğŸ“‰ ğŸ§  ğŸ“

"""
# %%
import pandas as pd
import numpy as np
import h5py
import os
from pathlib import Path
from functools import partial
import traceback
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from tqdm import tqdm
import toml


from utils.dirutils import load_path_config
from utils.market import *
from utils.speedutils import timeit
from utils.timeutils import generate_time_series_in_date_range


# %%
# time: 400s
def concatenate_features_from_parquet1(feature: str, ts_dir: Path, cs_dir: Path):
    feature_dir = ts_dir / feature
    symbols = extract_symbols(feature_dir, '.parquet')

    series_list = []
    
    for symbol in tqdm(symbols, desc=feature):
        series_path = feature_dir / f'{symbol}.parquet'
        series = pd.read_parquet(series_path)
        series.columns = [symbol]
        series_list.append(series)
    
    concatenated_df = pd.concat(series_list, axis=1)
    concatenated_df.to_parquet(cs_dir / f'{feature}.parquet')
    

# load : 1.5min fill: 3min
def concatenate_features_from_parquet2(feature: str, ts_dir: Path, cs_dir: Path, params: dict):
    """
    åˆå¹¶ç‰¹å¾æ•°æ®æ–‡ä»¶ï¼Œå¹¶åŸºäºæœ€å¤§å’Œæœ€å°æ—¥æœŸåŠ¨æ€ç”Ÿæˆå…¨å±€ç´¢å¼•ã€‚
    
    :param feature: ç‰¹å¾åç§°
    :param ts_dir: æ—¶é—´åºåˆ—æ–‡ä»¶è·¯å¾„
    :param cs_dir: åˆå¹¶åæ–‡ä»¶ä¿å­˜è·¯å¾„
    :param params: æ—¶é—´é—´éš”å‚æ•° (å­—å…¸å½¢å¼ï¼Œå¦‚ {'seconds': 1})
    """
    feature_dir = ts_dir / feature
    symbols = extract_symbols(feature_dir, '.parquet')

    # Step 1: é¢„åŠ è½½æ‰€æœ‰æ–‡ä»¶åˆ°å†…å­˜å¹¶åŠ¨æ€æ›´æ–°æœ€å¤§å’Œæœ€å°æ—¥æœŸ
    data_dict = {}
    min_date, max_date = None, None

    for symbol in tqdm(symbols, desc=f"Loading data for {feature}"):
        series_path = feature_dir / f'{symbol}.parquet'
        series = pd.read_parquet(series_path)
        data_dict[symbol] = series

        # å°†ç´¢å¼•è½¬ä¸ºæ—¥æœŸ
        current_min_date = series.index[0].date()
        current_max_date = series.index[-1].date()

        # æ›´æ–°æœ€å°æ—¥æœŸå’Œæœ€å¤§æ—¥æœŸ
        if min_date is None or current_min_date < min_date:
            min_date = current_min_date
        if max_date is None or current_max_date > max_date:
            max_date = current_max_date

    # ç¡®ä¿æ—¥æœŸèŒƒå›´æœ‰æ•ˆ
    if min_date is None or max_date is None:
        raise ValueError("No valid data found in the input files.")

    # Step 2: ä½¿ç”¨ç”Ÿæˆå…¨å±€æ—¶é—´åºåˆ—ç´¢å¼•
    global_index = generate_time_series_in_date_range(
        pd.Timestamp(min_date), pd.Timestamp(max_date), params
    )

    # Step 3: åˆ›å»ºä¸€ä¸ªç©ºçš„ DataFrame
    concatenated_df = pd.DataFrame(index=global_index, columns=symbols, dtype=float)

    # Step 4: å¡«å…… DataFrame
    for symbol in tqdm(symbols, desc=f"Filling data for {feature}"):
        series = data_dict[symbol].reindex(global_index)
        concatenated_df[symbol] = series  # å‡è®¾æ¯ä¸ªæ–‡ä»¶åªæœ‰ä¸€åˆ—

    # Step 5: ä¿å­˜ç»“æœ
    concatenated_df.to_parquet(cs_dir / f'{feature}.parquet')


def read_parquet_file(file_path: Path):
    """è¯»å–å•ä¸ª parquet æ–‡ä»¶å¹¶è¿”å› DataFrame"""
    series = pd.read_parquet(file_path)
    return series

def reindex_series(series, global_index):
    """å¯¹å•ä¸ª series è¿›è¡Œ reindex"""
    return series.reindex(global_index)

# load : 10s fill: 40s
def concatenate_features_from_parquet3(feature: str, ts_dir: Path, cs_dir: Path, params: dict, max_workers: int = 4):
    """
    åˆå¹¶ç‰¹å¾æ•°æ®æ–‡ä»¶ï¼Œå¹¶åŸºäºæœ€å¤§å’Œæœ€å°æ—¥æœŸåŠ¨æ€ç”Ÿæˆå…¨å±€ç´¢å¼•ã€‚
    
    :param feature: ç‰¹å¾åç§°
    :param ts_dir: æ—¶é—´åºåˆ—æ–‡ä»¶è·¯å¾„
    :param cs_dir: åˆå¹¶åæ–‡ä»¶ä¿å­˜è·¯å¾„
    :param params: æ—¶é—´é—´éš”å‚æ•° (å­—å…¸å½¢å¼ï¼Œå¦‚ {'seconds': 1})
    :param max_workers: æœ€å¤§çº¿ç¨‹æ•°
    """
    feature_dir = ts_dir / feature
    symbols = extract_symbols(feature_dir, '.parquet')

    # Step 1: ä½¿ç”¨å¤šçº¿ç¨‹é¢„åŠ è½½æ‰€æœ‰æ–‡ä»¶åˆ°å†…å­˜å¹¶åŠ¨æ€æ›´æ–°æœ€å¤§å’Œæœ€å°æ—¥æœŸ
    data_dict = {}
    min_date, max_date = None, None

    file_paths = {symbol: feature_dir / f'{symbol}.parquet' for symbol in symbols}

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(read_parquet_file, path): symbol for symbol, path in file_paths.items()}

        for future in tqdm(as_completed(futures), total=len(futures), desc=f"Loading data for {feature}"):
            symbol = futures[future]
            series = future.result()
            data_dict[symbol] = series

            # å°†ç´¢å¼•è½¬ä¸ºæ—¥æœŸ
            current_min_date = series.index[0].date()
            current_max_date = series.index[-1].date()

            # æ›´æ–°æœ€å°æ—¥æœŸå’Œæœ€å¤§æ—¥æœŸ
            if min_date is None or current_min_date < min_date:
                min_date = current_min_date
            if max_date is None or current_max_date > max_date:
                max_date = current_max_date

    # ç¡®ä¿æ—¥æœŸèŒƒå›´æœ‰æ•ˆ
    if min_date is None or max_date is None:
        raise ValueError("No valid data found in the input files.")

    # Step 2: ä½¿ç”¨ç”Ÿæˆå…¨å±€æ—¶é—´åºåˆ—ç´¢å¼•
    global_index = generate_time_series_in_date_range(
        pd.Timestamp(min_date), pd.Timestamp(max_date), params
    )

    # Step 3: åˆ›å»ºä¸€ä¸ªç©ºçš„ DataFrame
    concatenated_df = pd.DataFrame(index=global_index, columns=symbols, dtype=float)

    # Step 4: å¡«å…… DataFrame
# =============================================================================
#     for symbol in tqdm(symbols, desc=f"Filling data for {feature}"):
#         series = data_dict[symbol].reindex(global_index)
#         concatenated_df[symbol] = series  # å‡è®¾æ¯ä¸ªæ–‡ä»¶åªæœ‰ä¸€åˆ—
#         # series = data_dict[symbol]
#         # concatenated_df.loc[series.index, symbol] = series.iloc[:, 0]  # å‡è®¾æ¯ä¸ªæ–‡ä»¶åªæœ‰ä¸€åˆ—
# =============================================================================
        
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(reindex_series, data_dict[symbol], global_index): symbol
            for symbol in symbols
        }

        for future in tqdm(as_completed(futures), total=len(futures), desc=f"Reindexing data for {feature}"):
            symbol = futures[future]  # è·å–å½“å‰ä»»åŠ¡å¯¹åº”çš„ symbol
            reindexed_series = future.result()
            concatenated_df[symbol] = reindexed_series
            
    # Step 5: ä¿å­˜ç»“æœ
    concatenated_df.to_parquet(cs_dir / f'{feature}.parquet')

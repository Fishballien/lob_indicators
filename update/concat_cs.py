# -*- coding: utf-8 -*-
"""
Created on Mon Dec 16 14:32:02 2024

@author: Xintang Zheng

æ˜Ÿæ˜Ÿ: â˜… â˜† âœª âœ© ğŸŒŸ â­ âœ¨ ğŸŒ  ğŸ’« â­ï¸
å‹¾å‹¾å‰å‰: âœ“ âœ” âœ• âœ– âœ… â
æŠ¥è­¦å•¦: âš  â“˜ â„¹ â˜£
ç®­å¤´: â” âœ â™ â¤ â¥ â†© â†ª
emoji: ğŸ”” â³ â° ğŸ”’ ğŸ”“ ğŸ›‘ ğŸš« â— â“ âŒ â­• ğŸš€ ğŸ”¥ ğŸ’§ ğŸ’¡ ğŸµ ğŸ¶ ğŸ§­ ğŸ“… ğŸ¤” ğŸ§® ğŸ”¢ ğŸ“Š ğŸ“ˆ ğŸ“‰ ğŸ§  ğŸ“

"""
# %% imports
import sys
from pathlib import Path
import pandas as pd
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
import traceback


# %% add sys path
file_path = Path(__file__).resolve()
file_dir = file_path.parents[0]
project_dir = file_path.parents[1]
sys.path.append(str(project_dir))


# %% import self_defined
from utils.dirutils import get_filenames_by_extension
from utils.logutils import FishStyleLogger
from utils.datautils import add_dataframe_to_dataframe_reindex, check_dataframe_consistency


# %%
class CheckNUpdate: # TODO: å¯ä»¥æŒ‰åŠŸèƒ½æ‹†åˆ†å‡ºåŸºç±»
    
    def __init__(self, historical_dir, incremental_dir, updated_dir, target_names=None, 
                 check_consistency=True, n_workers=1, log=None):
        self.historical_dir = historical_dir
        self.incremental_dir = incremental_dir
        self.updated_dir = updated_dir
        self.target_names = target_names
        self.check_consistency = check_consistency
        self.n_workers = n_workers
        self.log = log or FishStyleLogger()
        
    def run(self):
        self._get_factor_names()
        self._decide_pre_update_dir()
        status = self._update_all_factors()
        return status
        
    def _get_factor_names(self):
        self.factor_names = self.target_names or get_filenames_by_extension(self.historical_dir, '.parquet')
        
    def _decide_pre_update_dir(self):
        self.updated_dir.mkdir(parents=True, exist_ok=True)
        updated_factor_names = get_filenames_by_extension(self.updated_dir, '.parquet')
        
        if all([fac in updated_factor_names for fac in self.factor_names]):
            self.pre_updated_dir = self.updated_dir
        else:
            self.pre_updated_dir = self.historical_dir
            self.log.warning('æ›´æ–°æ–‡ä»¶æ•°é‡ä¸è¶³ï¼Œä½¿ç”¨å†å²æ–‡ä»¶åšæ›´æ–°ï¼')
            
    def _update_all_factors(self):
        """ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘æ›´æ–°æ‰€æœ‰å› å­ï¼Œé‡åˆ°é”™è¯¯æ—¶ç«‹å³ä¸­æ­¢ç¨‹åº"""
        len_of_factors = len(self.factor_names)
        with ThreadPoolExecutor(max_workers=min(self.n_workers, len_of_factors)) as executor:
            futures = {executor.submit(self._update_one_factor, factor_name): 
                       factor_name for factor_name in self.factor_names}
            
            for future in as_completed(futures):
                factor_name = futures[future]
                try:
                    future.result()  # è·å–çº¿ç¨‹çš„è¿”å›ç»“æœï¼Œç¡®ä¿å¼‚å¸¸è¢«æ•è·
                except Exception as exc:
                    self.log.error(f'{factor_name} æ›´æ–°è¿‡ç¨‹ä¸­å‡ºé”™: {exc}')
                    # è®°å½•å®Œæ•´çš„é”™è¯¯å †æ ˆ
                    error_traceback = traceback.format_exc()
                    self.log.error(f'é”™è¯¯å †æ ˆ: {error_traceback}')
                    # ç«‹å³ä¸­æ­¢ç¨‹åº
                    raise RuntimeError(f"å› å­ {factor_name} æ›´æ–°å¤±è´¥ï¼Œç¨‹åºä¸­æ­¢") from exc
    
    def _update_one_factor(self, factor_name):
        pre_updated_dir = self.pre_updated_dir
        incremental_dir = self.incremental_dir
        updated_dir = self.updated_dir

        file_name = f'{factor_name}.parquet'
        
        pre_update_path = pre_updated_dir / file_name
        incremental_path = incremental_dir / file_name
        pre_update_data = pd.read_parquet(pre_update_path)
        incremental_data = pd.read_parquet(incremental_path)
        updated_data = self._check_n_update(factor_name, pre_update_data, incremental_data)
        
        updated_path = updated_dir / file_name
        updated_data.to_parquet(updated_path)
    
    def check_n_update(self, factor_name, pre_update_data, incremental_data):
        """
        æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§å¹¶æ›´æ–°æ•°æ®å¸§
        
        å‚æ•°:
        factor_name (str): å› å­åç§°ï¼Œç”¨äºé”™è¯¯ä¿¡æ¯
        pre_update_data (pd.DataFrame): å¾…æ›´æ–°çš„æ•°æ®å¸§
        incremental_data (pd.DataFrame): å¢é‡æ•°æ®å¸§
        
        è¿”å›å€¼:
        pd.DataFrame: æ›´æ–°åçš„æ•°æ®å¸§
        
        å¼‚å¸¸:
        ValueError: å½“æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥æ—¶æŠ›å‡º
        """
        # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        if self.check_consistency:
            status, info = check_dataframe_consistency(pre_update_data, incremental_data)
            
            if status == "INCONSISTENT":
                # æ„é€ é”™è¯¯ä¿¡æ¯
                error_msg = f"å› å­[{factor_name}]æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥! ç´¢å¼•: {info['index']}, åˆ—: {info['column']}, "
                error_msg += f"åŸå§‹å€¼: {info['original_value']}, æ–°å€¼: {info['new_value']}, "
                error_msg += f"ä¸ä¸€è‡´è®¡æ•°: {info['inconsistent_count']}"
                
                raise ValueError(error_msg)
            
        # å¦‚æœä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡ï¼Œç»§ç»­æ›´æ–°æ•°æ®
        updated_data = self._update_to_updated(pre_update_data, incremental_data)
        
        return updated_data

    def _update_to_updated(self, pre_update_data, incremental_data):
        updated_data = add_dataframe_to_dataframe_reindex(
            pre_update_data, incremental_data)
        
        return updated_data
    
    
def run_concat(config_name):
    """
    Run concatenation process using the specified configuration.
    
    Args:
        config_name (str): Name of the configuration file (without .toml extension)
    """
    path_config = load_path_config(project_dir)
    config_dir = Path(path_config['param']) / 'concat'
    config = toml.load(config_dir / f'{config_name}.toml')
    
    check_consistency = config['check_consistency']
    n_workers = config['n_workers']
    
    for concat_info in config['concat_target']:
        historical_dir = Path(concat_info['historical_dir'])
        incremental_dir = Path(concat_info['incremental_dir'])
        updated_dir = Path(concat_info['updated_dir'])
        target_names = concat_info.get('target_names')
        
        if target_names is not None:
            target_names = [f'{side}_{factor}' for factor in target_names for side in ('Bid', 'Ask')]
        
        c = CheckNUpdate(
            historical_dir, 
            incremental_dir, 
            updated_dir, 
            target_names=target_names,
            check_consistency=check_consistency, 
            n_workers=n_workers
        )
        c.run()
    
    
# %%
if __name__ == "__main__":
    import toml
    from utils.dirutils import load_path_config
    
    config_name = 'concat_for_xkq'

    path_config = load_path_config(project_dir)
    config_dir = Path(path_config['param']) / 'concat'
    config = toml.load(config_dir / f'{config_name}.toml')
    
    check_consistency = config['check_consistency']
    n_workers = config['n_workers']

    for concat_info in config['concat_target']:
        historical_dir = Path(concat_info['historical_dir'])
        incremental_dir = Path(concat_info['incremental_dir'])
        updated_dir = Path(concat_info['updated_dir'])
        target_names = concat_info.get('target_names')
        if target_names is not None:
            target_names = [f'{side}_{factor}' for factor in target_names for side in ('Bid', 'Ask')]
        
        c = CheckNUpdate(historical_dir, incremental_dir, updated_dir, target_names=target_names, 
                         check_consistency=check_consistency, n_workers=n_workers)
        c.run()
        